---
title: "Data Science Milestone Report"
author: "Joe Yadush"
date: "November 19, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringi)
library(tm)
library(stringr)
library(dplyr)
library(RWeka)
library(ggplot2)
```

## Overview
This is a progress report for the Coursera Data Science Captsone. The intent of this interim report is to 
demonstrate the ability to download selected text from various social media, load into R and do some initial cleaning and exploration with the intent of working towards  building a predictive natural programming language algorithm.

## Obtaining the dataset
For the project the source data will be downloaded and intstalled on a local machine first checking if data exists already or not
```{r}
if (!file.exists("capstone")){
        dir.create("capstone")
}
sourceUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
sourcefile <- "./capstone/Coursera-Swiftkey.zip"
if (!file.exists(sourcefile)){
        download.file(sourceUrl, destfile = "./capstone/Coursera-Swiftkey.zip")
        unzip(sourcefile, exdir = "./capstone")
}
```

Examining the data sets to understand what we have to work with:
```{r echo=FALSE}
blogData    <- readLines("./capstone/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
twitterData <- readLines("./capstone/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
con <- file("final/en_US/en_US.news.txt", open="rb")
newsData    <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
rm(con)
mb <- 1024 ^2
blogData.size     <-file.info("./capstone/final/en_US/en_US.blogs.txt")$size / mb
twitterData.size  <-file.info("./capstone/final/en_US/en_US.twitter.txt")$size / mb
newsData.size     <-file.info("./capstone/final/en_US/en_US.news.txt")$size / mb

# number of words
blogData.words     <-stri_count_words(blogData)
twitterData.words  <-stri_count_words(twitterData)
newsData.words     <-stri_count_words(newsData)

# build a dataframe to display summarization
dfSummary <- data.frame(source = c("blogs", "twitter", "news"),
                        file.MB    = c(blogData.size, twitterData.size, newsData.size),
                        file.words = c(sum(blogData.words), sum(twitterData.words), sum(newsData.words)),
                        file.lines = c(length(blogData), length(twitterData), length(newsData)),
                        avg.words  = c(c(mean(blogData.words), mean(twitterData.words), mean(newsData.words))))

```
```{r}
dfSummary
```
## Cleaning the data
The files are text and as the sources are from various media such as blogs or twitter, we must attempt to
first remove special characters such as emojis, hyperlinks, numbers, etc. Also, as shown in the summary table, some
of the files are enormous and for sampling we will need to take a subset of the data only to which to work with
to be both representative as well as not to overload a machines' resources.

Method will be to take a small sample of all three files into a single corpus and then clean the data
```{r echo=FALSE}
# create sample
set.seed(456)
sampleData<- c(sample(blogData, length(blogData) * 0.003),
                 sample(twitterData, length(twitterData) * 0.001),
                 sample(newsData, length(newsData) * 0.001))
# transform data jush as emojis
sampleData <- iconv(sampleData, "UTF-8", "ASCII")
sampleData[which(is.na(sampleData))] <-"NULLVALUES"
# build corpus
sampleCorpus <- VCorpus(VectorSource(sampleData))
toSpace <-content_transformer(function(x, pattern) gsub(pattern, " ", x))
# clean the data by building a small sample corpus in memory and then removing special characters
# and web sites
sampleCorpus <- tm_map(sampleCorpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
sampleCorpus <- tm_map(sampleCorpus, toSpace, "@[^\\s]+")
sampleCorpus <- tm_map(sampleCorpus, toSpace, "NULLVALUES")
sampleCorpus <- tm_map(sampleCorpus, tolower)
sampleCorpus <- tm_map(sampleCorpus, removeWords, stopwords("en"))
sampleCorpus <- tm_map(sampleCorpus, removePunctuation)
sampleCorpus <- tm_map(sampleCorpus, removeNumbers)
sampleCorpus <- tm_map(sampleCorpus, stripWhitespace)
sampleCorpus <- tm_map(sampleCorpus, PlainTextDocument)
```
## Exploring the data
The goal is to ultimately use natural language processing to predict text so the choice was
made ot create 1,2 & 3n-grams from the sample and explore what the most common text was
```{r echo=FALSE}
uniToken <- function(x) {NGramTokenizer(x, Weka_control(min = 1, max = 1))}
unigram  <- DocumentTermMatrix(sampleCorpus, control = list(tokenize = uniToken))
unigram_freq <- sort(colSums(as.matrix(unigram)), decreasing = TRUE)
unigram_freq_df <- data.frame(word = names(unigram_freq), frequency = unigram_freq)


biToken <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
bigram  <- DocumentTermMatrix(sampleCorpus, control = list(tokenize = biToken))
bigram_freq <- sort(colSums(as.matrix(bigram)), decreasing = TRUE)
bigram_freq_df <- data.frame(word = names(bigram_freq), frequency = bigram_freq)


triToken <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}
trigram  <- DocumentTermMatrix(sampleCorpus, control = list(tokenize = triToken))
trigram_freq <- sort(colSums(as.matrix(trigram)), decreasing = TRUE)
trigram_freq_df <- data.frame(word = names(trigram_freq), frequency = trigram_freq)

# plot
par(mfrow = c(3,1), mar = c(4,4,2,1))
unigram_freq_df %>% 
        filter(frequency > 200) %>%
        ggplot(aes(reorder(word,-frequency), frequency)) +
        geom_bar(stat = "identity") +
        ggtitle("Top Unigrams") +
        xlab("Unigrams") + ylab("Frequency")

bigram_freq_df %>% 
        filter(frequency > 15) %>%
        ggplot(aes(reorder(word,-frequency), frequency)) +
        geom_bar(stat = "identity") +
        ggtitle("Top biigrams") +
        xlab("biigrams") + ylab("Frequency")

trigram_freq_df %>% 
        filter(frequency > 3) %>%
        ggplot(aes(reorder(word,-frequency), frequency)) +
        theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
        geom_bar(stat = "identity") +
        ggtitle("Top trigrams") +
        xlab("trigrams") + ylab("Frequency")
```

## Observations and Next Steps
Sample size was very limited due to machine resources so may need to explore additional options,
but initial observations on what could be sampled shows some interesting opportunities between the bigram and ngram samples. It is a little unclear yet understaning a single word frequency by istlef will be useful, with the caveat the sample is a combined set of records from 3 sources which have very different writig styles.

Next steps are to work a predictive algorithm and train the data wih the data product being a shiny app allowing a simple input of text with confident, predcitive suggestions to continue the text.

